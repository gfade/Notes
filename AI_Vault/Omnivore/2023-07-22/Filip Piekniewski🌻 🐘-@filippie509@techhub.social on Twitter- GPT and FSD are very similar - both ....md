---
id: 9c4eb2c7-3b8d-4adf-9a09-69a10683cf40
---

# Filip Piekniewskiüåª üêò:@filippie509@techhub.social on Twitter: GPT and FSD are very similar - both produce great social media content, both can fail unexpectedl...
#Omnivore

[Read on Omnivore](https://omnivore.app/me/https-twitter-com-filippie-509-status-1639292872917946369-1897da3a825)
[Read Original](https://twitter.com/filippie509/status/1639292872917946369)

GPT and FSD are very similar - both produce great social media content, both can fail unexpectedly even on the silliest of problems. FSD can kill you, GPT may embarrass you. The [#AI](https://twitter.com/search?q=%23AI) we apparently deserve.

Also both three letter acronyms have a devoted army of fanboys, who filter out all the failures and blow all the successes creating a giant selection bias on social media which leaves everyone else flabbergasted.

The reasons why GPT or FSD fail are actually very similar - when they are correct, they are correct for the wrong reasons. We are under illusion these things can see or understand language the same way we do. But that is completely not the case.

These thing rely on statistics. We rely on "mechanics". Arguably brains do use statistics to discover the mechanics, but we go much further than these models. Discovering mechanics removes spurious correlation. These models on the other hand love spurious correlation.

A quick illustration: when I drive my kids to school I pass an office building. There is an intersection with the driveway to the building, but unless there is anyone leaving or entering the driveway, the light is always green to go forward. Except on Tuesday. On Tuesday it

is very frequently red. An ML-like model would very quickly pick up that correlation, it is really pretty strong. You can bet if it is Tuesday you will be waiting on a red light on that intersection at least 50% of the time. But WHY?

The reason is on Tuesday there are gardeners parked with their truck over the induction loop that triggers the signals. That is the mechanics of what is observed. So next time the gardeners move their schedule because of e.g. the weather, and they come on Wednesday,

a human will immediately infer that it is now likely that red light will appear on Wednesday. That there was nothing magical about Tuesday, the correlation may have been strong, but the CAUSE was different. ML system would have to first unlearn the Tue and then re-learn Wed.

No ML system is able to perform that step from "statistics" to "mechanics". We sometimes do that for these systems by engineering features, and filtering outputs. But on their own these systems are clueless because they have no access or incentive to model the dynamics that

creates the signals they work with. And that is the crux of the problem. There is no transcendence from "signal statistics" to the dynamics that generated that signal. Or at least it is very rudimentary with these ML contraptions.

 ‚Äî [filippie509](https://twitter.com/filippie509) Filip Piekniewskiüåª üêò:@filippie509@techhub.social [March 24, 2023, 2:09 PM UTC](https://twitter.com/filippie509/status/1639292872917946369) 


