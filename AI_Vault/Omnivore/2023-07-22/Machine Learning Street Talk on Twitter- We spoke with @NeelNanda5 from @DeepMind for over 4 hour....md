---
id: 28573caf-f70e-4cac-8f23-1de05c018cd3
---

# Machine Learning Street Talk on Twitter: We spoke with @NeelNanda5 from @DeepMind for over 4 hours (!!) about Mechanistic Interpretability...
#Omnivore

[Read on Omnivore](https://omnivore.app/me/https-twitter-com-ml-street-talk-status-1670430070458966016-1897da11e51)
[Read Original](https://twitter.com/MLStreetTalk/status/1670430070458966016)

We spoke with [@NeelNanda5](https://twitter.com/NeelNanda5) from @DeepMind for over 4 hours (!!) about Mechanistic Interpretability. Enjoy ðŸ˜€ [podcasters.spotify.com/pod/sâ€¦](https://podcasters.spotify.com/pod/show/machinelearningstreettalk/episodes/Neel-Nanda---Mechanistic-Interpretability-e25sibc) We will release the video version as soon as possible.

[ ![](https://proxy-prod.omnivore-image-cache.app/0x0,sBDyYQtwCtuQsbaYFw15rgjA92ROU0kE7HVyayDGBM40/https://pbs.twimg.com/media/Fy6Oug0X0AA6AdY.jpg?name=small&format=webp) ](https://pbs.twimg.com/media/Fy6Oug0X0AA6AdY.jpg?name=small&format=webp)

In this wide-ranging conversation, Tim Scarfe interviews Neel Nanda, a researcher at DeepMind working on mechanistic interpretability, which aims to understand the algorithms and representations learned by machine learning models. Neel discusses how models can represent their thoughts using motifs, circuits, and linear directional features which are often communicated via a "residual stream", an information highway models use to pass information between layers. Neel argues that "superposition", the ability for models to represent more features than they have neurons, is one of the biggest open problems in interpretability. This is because superposition thwarts our ability to understand models by decomposing them into individual units of analysis. Despite this, Neel remains optimistic that ambitious interpretability is possible, citing examples like his work reverse engineering how models do modular addition. However, Neel notes we must start small, build rigorous foundations, and not assume our theoretical frameworks perfectly match reality. The conversation turns to whether models can have goals or agency, with Neel arguing they likely can based on heuristics like models executing long term plans towards some objective. However, we currently lack techniques to build models with specific goals, meaning any goals would likely be learned or emergent. Neel highlights how induction heads, circuits models use to track long range dependencies, seem crucial for phenomena like in-context learning to emerge. On the existential risks from AI, Neel believes we should avoid overly confident claims that models will or will not be dangerous, as we do not understand them enough to make confident theoretical assertions. However, models could pose risks through being misused, having undesirable emergent properties, or being imperfectly aligned. Neel argues we must pursue rigorous empirical work to better understand and ensure model safety, avoid "philosophizing" about definitions of intelligence, and focus on ensuring researchers have standards for what it means to decide a system is "safe" before deploying it. Overall, a thoughtful conversation on one of the most important issues of our time.

TOC: \[00:00:00\] Introduction and Neel Nanda's Interests (walk and talk) \[00:03:15\] Mechanistic Interpretability: Reverse Engineering Neural Networks \[00:13:23\] Discord questions \[00:21:16\] Main interview kick-off in studio \[00:49:26\] Grokking and Sudden Generalization \[00:53:18\] The Debate on Systematicity and Compositionality \[01:19:16\] How do ML models represent their thoughts \[01:25:51\] Do Large Language Models Learn World Models? \[01:53:36\] Superposition and Interference in Language Models \[02:43:15\] Transformers discussion \[02:49:49\] Emergence and In-Context Learning \[03:20:02\] Superintelligence/XRisk discussion

Refs: Toy Models of Superposition[dynalist.io/d/n2ZWtnoYHrU1s4â€¦](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#inline-images&theme=default) [](https://transformer-circuits.pub/2022/toy%5Fmodel/index.html)[](https://transformer-circuits.pub/2021/framework/index.html)[transformer-circuits.pub/202â€¦](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) [youtube.com/watch?v=R3nbXgMnâ€¦](https://www.youtube.com/watch?v=R3nbXgMnVqQ) (Nanda) Supermasks in Superposition[arxiv.org/abs/2006.14769](https://arxiv.org/abs/2006.14769)INTERPRETABILITY IN THE WILD[arxiv.org/pdf/2211.00593.pdf](https://arxiv.org/pdf/2211.00593.pdf)Actually, Othello-GPT Has A Linear Emergent World Representation (Nanda)[lesswrong.com/s/nhGNHyJHbrofâ€¦](https://www.lesswrong.com/s/nhGNHyJHbrofpPbRG/p/nmxzr2zsjNtjaHh7x) [thegradient.pub/othello](https://thegradient.pub/othello)A Mathematical Framework for Transformer Circuits transformer-circuits.pub/202â€¦[youtube.com/watch?v=KV5gbOmHâ€¦](https://www.youtube.com/watch?v=KV5gbOmHbjU) (Nanda) Attention is all you need[arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING[openreview.net/forum?id=BJC\_â€¦](https://openreview.net/forum?id=BJC%5FjUqxe)Distributed Representations of Words and Phrases and their Compositionality (Mikolov)[arxiv.org/abs/1310.4546](https://arxiv.org/abs/1310.4546)Deep Residual Learning for Image Recognition[arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)Attribution Patching: Activation Patching At Industrial Scale (Nanda)[neelnanda.io/mechanistic-intâ€¦](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)In-context Learning and Induction Heads transformer-circuits.pub/202â€¦[youtube.com/watch?v=dCkQQYwPâ€¦](https://www.youtube.com/watch?v=dCkQQYwPxdM) (Nanda) The Quantization Model of Neural Scaling[arxiv.org/pdf/2303.13506.pdf](https://arxiv.org/pdf/2303.13506.pdf)Interpreting Neural Networks to Improve Politeness Comprehension[aclanthology.org/D16-1216/](https://aclanthology.org/D16-1216/)Progress measures for grokking via mechanistic interpretability[arxiv.org/abs/2301.05217](https://arxiv.org/abs/2301.05217) (Nanda)[youtube.com/watch?v=IHikLL8Uâ€¦](https://www.youtube.com/watch?v=IHikLL8ULa4) [singapore.unofficialbird.com/NeelNanda5/statuâ€¦](https://singapore.unofficialbird.com/NeelNanda5/status/1616590887873839104)Grokking paper[arxiv.org/abs/2201.02177](https://arxiv.org/abs/2201.02177)A Toy Model of Universality[arxiv.org/abs/2302.03025](https://arxiv.org/abs/2302.03025) [singapore.unofficialbird.com/bilalchughtai\_/sâ€¦](https://singapore.unofficialbird.com/bilalchughtai%5F/status/1625948104121024516)A circuit for Python docstrings in a 4-layer attention-only transformer[lesswrong.com/posts/u6KXXmKFâ€¦](https://www.lesswrong.com/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only)Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: A critical analysis.[psycnet.apa.org/record/1989-â€¦](https://psycnet.apa.org/record/1989-03804-001)Maximal Update Parametrization (Î¼P) and Hyperparameter Transfer (Î¼Transfer)[github.com/microsoft/mup](https://github.com/microsoft/mup)Spline theory of NNs[proceedings.mlr.press/v80/baâ€¦](https://proceedings.mlr.press/v80/balestriero18b/balestriero18b.pdf)Counterarguments to the basic AI x-risk case (Katja Grace)[lesswrong.com/posts/LDRQ5Zfqâ€¦](https://www.lesswrong.com/posts/LDRQ5Zfqwi8GjzPYG/counterarguments-to-the-basic-ai-x-risk-case)The alignment problem from a deep learning perspective (Ngo)[arxiv.org/abs/2209.00626](https://arxiv.org/abs/2209.00626)Superintelligence: Paths, Dangers, Strategies.[amazon.co.uk/Superintelligenâ€¦](https://www.amazon.co.uk/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111)

Entire transcript: [docs.google.com/document/d/1â€¦](https://docs.google.com/document/d/1FK1OepdJMrqpFK-%5F1Q3LQN6QLyLBvBwWW%5F5z8WrS1RI/edit?usp=sharing)

 â€” [MLStreetTalk](https://twitter.com/MLStreetTalk) Machine Learning Street Talk [June 18, 2023, 1:54 PM UTC](https://twitter.com/MLStreetTalk/status/1670430070458966016) 


