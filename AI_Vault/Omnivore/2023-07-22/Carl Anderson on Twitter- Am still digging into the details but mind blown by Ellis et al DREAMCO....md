---
id: f5094f4d-4997-4310-ab7b-6fcaa0316a15
---

# Carl Anderson on Twitter: Am still digging into the details but mind blown by Ellis et al DREAMCODER paper. This feels like...
#Omnivore

[Read on Omnivore](https://omnivore.app/me/https-twitter-com-leaping-llamas-status-1624820924137541632-1897da2cf22)
[Read Original](https://twitter.com/LeapingLlamas/status/1624820924137541632)

Am still digging into the details but mind blown by Ellis et al DREAMCODER paper. This feels like progress to AGI [arxiv.org/abs/2006.08381](https://arxiv.org/abs/2006.08381)

.[@ykilcher](https://twitter.com/ykilcher) provides a really great breakdown of the paper. Now I don't feel so bad that I didn't understand how the compression step works so efficiently.[invidious.fdn.fr/watch?v=qtu0aSTD…](https://invidious.fdn.fr/watch?v=qtu0aSTDE2I&ab%5Fchannel=YannicKilcher)

Here is [@simon\_alford0](https://twitter.com/simon%5Falford0) discussing his master's thesis work applying DreamCoder on ARC. It solved 22 tasks but tended to falter but Simon provides some insight why and interesting directions forward[invidious.fdn.fr/watch?v=wyabkjna…](https://invidious.fdn.fr/watch?v=wyabkjnadng&ab%5Fchannel=MITCBMM)

There are several possible directions: improvement in guided search, improvement in abstraction/compression (such as using Stitch), and blending top-down & bottom-up deduction and making use of subproblems, or reevaluating the problem with partial solutions.

 — [LeapingLlamas](https://twitter.com/LeapingLlamas) Carl Anderson [February 11, 2023, 4:54 PM UTC](https://twitter.com/LeapingLlamas/status/1624820924137541632) 


